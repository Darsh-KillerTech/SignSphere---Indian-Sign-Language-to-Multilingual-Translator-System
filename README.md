# TEAM 35 - SignSphere---Indian-Sign-Language-to-Multilingual-Translator-System


TEAM_ID - 35: HandSpeak Dynamos

The below link contains the flutter files, used for the Android Application

https://drive.google.com/drive/folders/1uOtZdlMUXOmdJV_FDYlygSPqAx10QVIy?usp=drive_link


OVERVIEW

SignSphere is a real-time sign language and pose recognition system designed to enhance communication between the deaf and hearing communities. By leveraging artificial intelligence (AI) hand and pose detection modules, the system recognizes sign language gestures and body movements, converting them into text and speech. This innovation promotes inclusivity and independence by making communication seamless in environments where sign language is not widely understood.


FEATURES

Real-time hand and pose detection
Text-to-speech conversion for audible communication
Multilingual support for broader accessibility
AI-driven gesture classification using deep learning
Portable and scalable across various devices


PROBLEM STATEMENT

Deaf and hard-of-hearing individuals face communication barriers in daily life, especially in education, healthcare, and public services. Traditional solutions like human interpreters or captioning have limitations such as availability and inability to capture the nuances of sign language.


SOLUTION

SignSphere leverages AI-powered sign language recognition to provide real-time translation of sign gestures into text and speech. This eliminates the dependency on human interpreters and provides an instant communication solution.


PROJECT OBJECTIVES

Develop an AI-powered sign language recognition system
Bridge the communication gap between deaf individuals and the hearing community
Provide real-time and accurate translation of sign gestures into text and speech
Ensure inclusivity in education, healthcare, and customer service sectors


TECHNOLOGIIES USED

Programming Language: Python, HTML, CSS
Frameworks: YOLO, Flask TensorFlow, PyTorch
Hand & Pose Detection: MediaPipe
Text-to-Speech: pyttsx3
Translation Framework: Google TTS, DeepTranslator
Real-time Processing: OpenCV


SYSTEM ARCHITECTURE

Gesture Detection - MediaPipe extracts hand and body pose landmarks.
AI-based Gesture Classification – Deep learning model identifies the sign.
Text Conversion – The recognized sign is converted into text.
Speech Output – The text is processed through a text-to-speech engine.


HARDWARE and SOFTWARE REQUIREMENTS

Hardware:
Computer and mobile device with a camera
Sufficient processing power for AI computations
Microphone and speaker for speech output

Software:
Python (≥ 3.8),
MediaPipe,
OpenCV,
TensorFlow,
PyTorch,
Text-to-Speech Engine


APPLICATIONS

Education: Assisting deaf students by translating sign language into text and speech.

Healthcare: Enabling seamless communication between doctors and deaf patients.

Customer Service: Improving accessibility in retail, banking, and government services.

Public Services: Enhancing accessibility at airports, train stations, and offices.


FUTURE ENHANCEMENTS

Expand support for multiple sign languages
Improve gesture recognition accuracy using larger datasets
Develop mobile and web-based applications for broader accessibility
Introduce customizable AI models for personalized learning
